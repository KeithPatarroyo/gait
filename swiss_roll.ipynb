{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import itertools\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "import numpy as np\n",
    "\n",
    "import contextlib\n",
    "import os\n",
    "from importlib import reload\n",
    "\n",
    "import torchvision\n",
    "from torch import autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "plt.rcParams['axes.facecolor'] = 'white'\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "plt.rcParams['font.sans-serif'] = ['Lucida Grande']\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import renyi\n",
    "import utils\n",
    "from renyi import renyi_mixture_divergence, rbf_kernel, poly_kernel, generic_kernel\n",
    "from renyi import test_mixture_divergence, renyi_mixture_divergence_stable\n",
    "from renyi import renyi_mixture_divergence, renyi_sim_divergence, renyi_sim_divergence_stable\n",
    "from renyi import breg_mixture_divergence_stable, breg_sim_divergence_stable\n",
    "from renyi import breg_mixture_divergence, breg_sim_divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.set_default_dtype(torch.float64)\n",
    "\n",
    "class GeneratorFC(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_dims):\n",
    "        super(GeneratorFC, self).__init__()\n",
    "        self.layers = []\n",
    "\n",
    "        prev_dim = input_size\n",
    "        for hidden_dim in hidden_dims:\n",
    "            self.layers.append(nn.Linear(prev_dim, hidden_dim))\n",
    "            self.layers.append(nn.BatchNorm1d(hidden_dim))\n",
    "            self.layers.append(nn.LeakyReLU())\n",
    "            prev_dim = hidden_dim\n",
    "        self.layers.append(nn.Linear(prev_dim, output_size))\n",
    "\n",
    "        self.layer_module = nn.ModuleList(self.layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        for layer in self.layer_module:\n",
    "            out = layer(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets.samples_generator import make_swiss_roll\n",
    "\n",
    "d, z_dim, gen_hiddens = 2, 16, [256, 256]\n",
    "batch_size_p = 600\n",
    "batch_size_q = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = GeneratorFC(z_dim, d, gen_hiddens).cuda()\n",
    "\n",
    "g_optimizer = torch.optim.Adam(generator.parameters(), lr=1e-3, betas=(0.9, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1\n",
    "sigma_mult = 1\n",
    "\n",
    "# emp_probs = 2*torch.ones(1, batch_size//2).cuda()/batch_size\n",
    "emp_probs_p = torch.ones(1, batch_size_p).cuda()/batch_size_p\n",
    "emp_probs_q = torch.ones(1, batch_size_q).cuda()/batch_size_q\n",
    "\n",
    "kernel = lambda x, y: generic_kernel(x, y, lambda u, v: rbf_kernel(u, v, sigmas=[0.2 * sigma_mult], log=True))\n",
    "# kernel = lambda x, y: generic_kernel(x, y, lambda u, v: multiquad_kernel(u, v, sigma=0.2))\n",
    "# kernel = lambda x, y: generic_kernel(x, y, lambda u, v: poly_kernel(u, v, degree=2))\n",
    "\n",
    "D = lambda x, y: breg_mixture_divergence_stable(emp_probs_p, x, emp_probs_q, y, kernel, symmetric=True)\n",
    "\n",
    "# D = lambda x, y: renyi_mixture_divergence_stable(emp_probs_p, x, emp_probs_q, y, kernel, alpha,\n",
    "#                                                  use_avg=False, use_full=False, symmetric=True)\n",
    "# D = lambda x, y: test_mixture_divergence(emp_probs_p, x, emp_probs_q, y, kernel,\n",
    "#                                          symmetric=True, use_avg=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 - 0.0259\n",
      "200 - 0.0260\n",
      "400 - 0.0290\n",
      "600 - 0.0191\n",
      "800 - 0.0236\n",
      "1000 - 0.0238\n",
      "1200 - 0.0307\n",
      "1400 - 0.0194\n",
      "1600 - 0.0266\n",
      "1800 - 0.0251\n",
      "2000 - 0.0258\n",
      "2200 - 0.0279\n",
      "2400 - 0.0253\n",
      "2600 - 0.0247\n",
      "2800 - 0.0169\n",
      "3000 - 0.0179\n",
      "3200 - 0.0245\n",
      "3400 - 0.0240\n",
      "3600 - 0.0197\n",
      "3800 - 0.0377\n",
      "4000 - 0.0258\n"
     ]
    }
   ],
   "source": [
    "# for g in g_optimizer.param_groups:\n",
    "#     g['lr'] = 1e-6\n",
    "\n",
    "for _ in range(5000):\n",
    "    \n",
    "    g_optimizer.zero_grad()\n",
    "    \n",
    "    x_real = torch.Tensor(make_swiss_roll(batch_size_p, noise=0.1)[0][:, [0, 2]]/10).cuda()\n",
    "    x_real += 0.01 * torch.randn_like(x_real)\n",
    "    \n",
    "    z = torch.randn(batch_size_q, z_dim).cuda()\n",
    "    with contextlib.nullcontext():  # autograd.detect_anomaly():\n",
    "        x_fake = generator(z)\n",
    "\n",
    "#         x = x_real[:batch_size//2]\n",
    "#         x_prime = x_real[batch_size//2:]\n",
    "#         y = x_fake[:batch_size//2]\n",
    "#         y_prime = x_fake[batch_size//2:]\n",
    "\n",
    "#         loss = D(x, y) + D(x_prime, y) + D(x, y_prime) + D(x_prime, y_prime) - 2*D(y, y_prime)\n",
    "\n",
    "        loss = D(x_real, x_fake)\n",
    "#         loss = D(x_fake, x_real)\n",
    "\n",
    "        if _ % 200 == 0:\n",
    "            print(\"%d - %.4f\" % (_, loss.item()))\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "    g_optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_real = make_swiss_roll(500, noise=0.1)[0][:, [0, 2]]/10\n",
    "z = torch.randn(500, z_dim).cuda()\n",
    "x_fake = generator(z).data.cpu().numpy()\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.plot(x_real[:, 0], x_real[:, 1], 'g*', alpha=0.5, label='True', markersize=3)\n",
    "plt.plot(x_fake[:, 0], x_fake[:, 1], 'ro', alpha=0.5, label='Fake', markersize=3)\n",
    "plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.00), fancybox=True, shadow=True, ncol=2);\n",
    "\n",
    "cur_axes = plt.gca()\n",
    "cur_axes.axes.get_xaxis().set_visible(False)\n",
    "cur_axes.axes.get_yaxis().set_visible(False)\n",
    "\n",
    "cur_axes.spines['top'].set_visible(False)\n",
    "cur_axes.spines['right'].set_visible(False)\n",
    "cur_axes.spines['bottom'].set_visible(False)\n",
    "cur_axes.spines['left'].set_visible(False)\n",
    "\n",
    "plt.show()\n",
    "# plt.savefig('swiss_roll.pdf', dpi=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
